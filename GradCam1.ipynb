{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GradCam1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMvj8raRDRU3Ab75ZvoDaPI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TakafumiMatsuda/Python-Training/blob/master/GradCam1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-O_Xayn6o6V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#https://qiita.com/sasayabaku/items/fd8923cf0e769104cc95\n",
        "#初心者のGradCAM | CNNの視覚化\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import Tensor\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable \n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim.lr_scheduler import ExponentialLR\n",
        "\n",
        "from torchvision.datasets import CIFAR10 \n",
        "import torchvision.transforms as transforms\n",
        "#from model import CNNModel\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQgIe6hTcf-K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#ハイパーパラメータ\n",
        "cifar_data_root = './data'\n",
        "batch_size = 50\n",
        "epochs = 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ql8CIHjYckOA",
        "colab_type": "code",
        "outputId": "b2182a8f-f6a6-49b2-ff1b-776339420331",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "###データ準備\n",
        "#Transform定義\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor      (),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5 ))]\n",
        ")\n",
        "train_data = CIFAR10(root=cifar_data_root, download=True, train=True, transform=transform) \n",
        "test_data = CIFAR10(root=cifar_data_root, download=True, train=False, transform=transform) \n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=4)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=4)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWTWnXWM_krv",
        "colab_type": "code",
        "outputId": "4f01e750-57b9-413c-eb87-7bf5d0d153cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "###モデル定義\n",
        "###★★improveNet1★★###\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#画像は3チャネル(RGB)×32ピクセル×32ピクセル　※3チャネル: 元の画像を光の三原色に分解\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=0)  \n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2) #MAXプーリング層\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=0)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2) #MAXプーリング層\n",
        "        self.conv3 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=0) \n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=1) #MAXプーリング層\n",
        "        ######self.view = np.view(self.size(0), -1) #★失敗\n",
        "        #self.pool2 = nn.AvgPool2d(kernel_size=3) #GlobalAveragePooling\n",
        "        self.fc1 = nn.Linear(128 * 3 * 3, 120) #[512, 120]\n",
        "        self.fc2 = nn.Linear(120, 10) #10クラス #出力層\n",
        "\n",
        "    def forward(self, x):\n",
        "        #print(x.shape) #[50, 3, 32, 32](ker=3) [bachsize, channel, 32*32]\n",
        "        x = self.pool1(F.relu(self.conv1(x))) #convで32→30、poolで30→15(ker=3)　\n",
        "        #print(x.shape) #[50, 64, 15, 15](ker=3)\n",
        "        x = self.pool2(F.relu(self.conv2(x))) #convで15→13、poolで13→6\n",
        "        #print(x.shape) #[50, 128, 6, 6](ker=3)\n",
        "        x = self.pool3(F.relu(self.conv3(x))) #convで6→4、poolで4→2\n",
        "        #print(x.shape) #[50, 128, 2, 2]　←☆CAMを行うためにはここで画像サイズが1,1になっている(GlobalAveragePooling)ことが必要\n",
        "        #print('--------')\n",
        "        #x = x.view(-1, 128 * 2 * 2)\n",
        "        x = x.view(x.size(0), -1) #4次元(batch, 直前のoutputチャネル数, kernel, kernel)のデータを2次元(batch, output*out*ker*ker)にして全結合層に渡す\n",
        "        #print(x.shape) #[50, 512]\n",
        "        \n",
        "        x = F.relu(self.fc1(x))\n",
        "        #x = F.relu(self.fc2(x))\n",
        "        #print(x.shape) #[50, 120]\n",
        "\n",
        "        x = self.fc2(x)\n",
        "        #print(x.shape) #[50, (1, 1,) 10] (ker=5)\n",
        "\n",
        "        return x\n",
        "\n",
        "net = Net()\n",
        "net.to(device) #GPUに渡す"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (pool3): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
              "  (fc1): Linear(in_features=1152, out_features=120, bias=True)\n",
              "  (fc2): Linear(in_features=120, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBk18G5kz2oR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Net().to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
        "schedular = ExponentialLR(optimizer, gamma=0.95)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qIDVH5Tcokj",
        "colab_type": "code",
        "outputId": "af54f067-3dc4-41b7-a813-c6705567118e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "###学習\n",
        "for epoch in range(epochs):\n",
        "    \n",
        "    schedular.step()\n",
        "    \n",
        "    epoch_loss =  0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for i, samples in enumerate(train_loader):\n",
        "        data, labels = samples\n",
        "        \n",
        "        data = data.to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        outputs = model(data)\n",
        "        loss = criterion(outputs, labels) \n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "        predicted = outputs.max(1, keepdim=True)[1]\n",
        "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
        "        total += labels.size(0)\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print('{} epoch: Loss {}, Accuracy {}'.format((epoch+1),\n",
        "                               epoch_loss / len(train_loader),\n",
        "                               correct / total\n",
        "                               ))\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:122: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1 epoch: Loss 1.7809701092243195, Accuracy 0.34028\n",
            "2 epoch: Loss 1.605231929898262, Accuracy 0.41026\n",
            "3 epoch: Loss 1.5686793940067292, Accuracy 0.426\n",
            "4 epoch: Loss 1.5375849734544753, Accuracy 0.43786\n",
            "5 epoch: Loss 1.5080163995027542, Accuracy 0.45146\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZqtcEq3cpSF",
        "colab_type": "code",
        "outputId": "38b14e0e-fb30-40d3-ade0-cd7f26342e7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "###Grad-CAMによる可視化\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "\n",
        "#Flattenの実装(PyTorchには，多次元配列を1次元の配列に変換する．Flattenがないので実装)\n",
        "class Flatten(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Flatten, self).__init__()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return x.view(x.size(0), -1)\n",
        "\n",
        "#特徴設計部の最終出力 & 識別部の出力を取得\n",
        "feature_fn = torch.nn.Sequential(*list(model.children())[:-2]).to(device) #Sequentialにリストを渡してやる\n",
        "#print(list(model.children())[:-2]) #[:-2]: リストの最後の2つのを除いた要素（MaxPoolまで）\n",
        "#print(list(model.children())) \n",
        "#print(list(model.children())[:-1])\n",
        "print(feature_fn) \n",
        "\n",
        "classfier_fn = torch.nn.Sequential(*(list(model.children())[-2:-1] + [Flatten()]\n",
        "                                    + list(model.children())[-1:]\n",
        "                                    )).to(device)\n",
        "#print(list(model.children())[-2:-1]) #512*120\n",
        "#print([Flatten()])\n",
        "#print(list(model.children())[-1:])\n",
        "print(classfier_fn) \n",
        "\n",
        "#GradCAM\n",
        "def GradCam(img, c, feature_fn, classifier_fn): #(input, class label, 特徴量層, 全結合層)\n",
        "    #print('img = ',img.shape) #torch.Size([1, 3, 32, 32])\n",
        "    feats = feature_fn(img.to(device)) #img\n",
        "    _, N, H, W = feats.size()\n",
        "    #print(feats)\n",
        "    #print(feats.size()) #torch.Size([1, 128, 13, 13]) = print(feats.shape)　★ここが[1, 128 , 2, 2]になってないとおかしい！！\n",
        "    #print(feats.size(0)) #1\n",
        "    #print(feats.view(feats.size(0))) #shape '[1]' is invalid for input of size 21632\n",
        "    #print(feats.view(feats.size(0), -1)) #tensor([[-70.0952, -72.0486, -69.8751,  ..., -74.4457, -74.7196, -76.3472]], device='cuda:0', grad_fn=<ViewBackward>)\n",
        "    #view: 4次元(1, 直前のoutputチャネル数, kernel, kernel)のデータを2次元(batch, output*out*ker*ker)に [1, 128*13*13]=[1, 21632]\n",
        "    out = classifier_fn(feats.view(feats.size(0), -1))\n",
        "    c_score = out[0, c]\n",
        "    print(c_score)\n",
        "    grads = torch.autograd.grad(c_score, feats) #(input, output): inputに対するoutputの勾配の合計\n",
        "    print(grads)\n",
        "    w = grads[0][0].mean(-1).mean(-1) #全ピクセルについて平均\n",
        "    sal = torch.matmul(w, feats.view(N, H*W))\n",
        "    sal = F.relu(sal) #Reluを行う\n",
        "    sal = sal.view(H, W).cpu().detach().numpy()\n",
        "    sal = np.maximum(sal, 0)\n",
        "    return sal, out, c_score, feats, grads, w\n",
        "\n",
        "#全結合層では画像の位置に関連した情報を完全に失ってしまう。一方、入力に近いところでは抽象度の低い認識しか行われない。\n",
        "#CNNの部分の最終（全結合層の入力に当たる部分）に双方を満たす情報がある\n",
        "\n",
        "###Grand-CAM###\n",
        "#クラスごとの確率スコアへの影響が大きい画像箇所を微分係数（特徴量マップ*においてある画像箇所に微小変化を加えたときに確率スコアに生じる変化の大きさを表す係数）の平均化によって特定する\n",
        "#出力対象クラス以外の勾配を0にセットして対象クラスのみを1に設定する。\n",
        "#★前提となるアイディア：クラス判定に与える影響が大きい画像箇所は、確率スコアの微分係数も大きい！\n",
        "\n",
        "###CAMの欠点###\n",
        "#ネットワークの全結合層がGlobal Average Pooling(画像サイズが(1*1)になるプーリング)層に置き換わってなくてはならない。conv->global average->softmax\n",
        "#また最終層ではチャネル数 = クラス数となっている必要あり\n",
        "\n",
        "\n",
        "\n",
        "#*入力画像をCNNの特徴抽出器（全結合層より前の部分）に通して得られる特徴量を画像として出力した2次元マップ"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sequential(\n",
            "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (5): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
            ")\n",
            "Sequential(\n",
            "  (0): Linear(in_features=1152, out_features=120, bias=True)\n",
            "  (1): Flatten()\n",
            "  (2): Linear(in_features=120, out_features=10, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJUZYFgrcpXJ",
        "colab_type": "code",
        "outputId": "831e2c21-dbb6-40f9-8cee-ae7705874204",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 742
        }
      },
      "source": [
        "###GradCAMの実行\n",
        "#対象画像の取得\n",
        "input_index = 2\n",
        "input_data = test_loader.dataset[input_index][0]\n",
        "#print(len(input_data)) #3\n",
        "input_data = input_data.view(1, input_data.shape[0], input_data.shape[1], input_data.shape[2]).to(device)\n",
        "#print(input_data.shape[0])\n",
        "#print(input_data.shape[1])\n",
        "#print(input_data.shape[2])\n",
        "#print(input_data.__getitem__(0)[0].shape) #[32, 32]\n",
        "#print(len(input_data)) #1\n",
        "\n",
        "#上位2クラスの識別部の出力 & クラスラベルを取得\n",
        "pp, cc = torch.topk(nn.Softmax(dim=1)(model(input_data)), 2) \n",
        "#topk()は，上位kクラスのデータとクラスラベルを取得する関数\n",
        "#pp → CNNモデルの出力層の出力\n",
        "#cc → それに対応するクラスラベル\n",
        "\n",
        "#print(pp) #[[0.1103, 0.1036]]\n",
        "#print(cc) #5,3\n",
        "#print(cc[0])\n",
        "#print(cc[0][0]) #5\n",
        "\n",
        "#saliency mapの取得\n",
        "sal, out, c_score, feats, grads, w = GradCam(input_data, cc[0][0], feature_fn, classfier_fn)\n",
        "#sal → cc[0][0]で指定したクラスラベルにおける勾配のSaliency Map\n",
        "#cc[0][0]\n",
        "\n",
        "#取得したSaliency Mapを画像化\n",
        "img = input_data.permute(0, 2, 3, 1).view(input_data.shape[2], input_data.shape[3], input_data.shape[1]).cpu().numpy()\n",
        "img_sal = Image.fromarray(sal).resize(img.shape[0:2], resample=Image.LINEAR)\n",
        "\n",
        "#表示\n",
        "plt.imshow(img)\n",
        "plt.imshow(np.array(img_sal), alpha=0.5, cmap=\"jet\")\n",
        "plt.colorbar()\n",
        "\n",
        "\n",
        "#内積をとる行列のサイズが不正な場合: RuntimeError: size mismatch, m1: [1*21632]実際の入力　入力, m2: [512 x 120]定義した入力\n",
        "#self.fc1 = nn.Linear(128 * 2 * 2, 120) と定義した\n",
        "##########classiferのinputは512*120\n",
        "#Sequential((0): Linear(in_features=512, out_features=120, bias=True)\n",
        "#           (1): Flatten()\n",
        "#           (2): Linear(in_features=120, out_features=10, bias=True))\n",
        "#########\n",
        "#RuntimeError: non-empty 3D or 4D (batch mode) tensor expected for input"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor(6277.3975, device='cuda:0', grad_fn=<SelectBackward>)\n",
            "(tensor([[[[ 0.3545,  0.2361,  0.2792],\n",
            "          [ 0.2153,  0.3125,  0.2968],\n",
            "          [ 0.1083,  0.0903,  0.0377]],\n",
            "\n",
            "         [[ 0.3121,  0.4373,  0.6679],\n",
            "          [ 0.4953,  0.4055,  0.3506],\n",
            "          [ 0.3427,  0.3092,  0.2812]],\n",
            "\n",
            "         [[ 0.0913,  0.1146, -0.0196],\n",
            "          [ 0.0692,  0.0960, -0.0041],\n",
            "          [ 0.0201, -0.0404,  0.0864]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.2001,  0.0173, -0.1040],\n",
            "          [-0.0079,  0.1449,  0.2248],\n",
            "          [-0.2641, -0.0076,  0.3195]],\n",
            "\n",
            "         [[ 0.1612, -0.8022, -1.2881],\n",
            "          [ 0.4336,  0.0470, -0.1490],\n",
            "          [ 0.0137, -0.6759,  0.1796]],\n",
            "\n",
            "         [[-0.0406, -0.0379, -0.0991],\n",
            "          [ 0.0180,  0.0221, -0.0284],\n",
            "          [ 0.0546,  0.0481, -0.1114]]]], device='cuda:0'),)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.colorbar.Colorbar at 0x7fda6925e828>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 137
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAATEAAAD5CAYAAABPqQIFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/+nFAown4W0AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOfjGae7g2-h",
        "colab_type": "code",
        "outputId": "2db595b4-0626-416b-a18a-af08bb2830d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "#動かなかったやつ\n",
        "###モデル定義（動かない…）\n",
        "class CNNModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNNModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=5)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=3)\n",
        "        self.conv2 = nn.Conv2d(64, 32, kernel_size=5)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=3)\n",
        "        self.conv3 = nn.Conv2d(32, 16, kernel_size=5)\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=3)\n",
        "        self.fc1 = nn.Linear(90, 2000)\n",
        "        self.fc2 = nn.Linear(2000, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.pool1(F.relu(self.conv1(x)))\n",
        "        out = self.pool2(F.relu(self.conv2(out)))\n",
        "        out = self.pool3(F.relu(self.conv3(out)))\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = F.relu(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "net = Net()\n",
        "net.to(device) #GPUに渡す"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (conv3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (fc1): Linear(in_features=512, out_features=120, bias=True)\n",
              "  (fc2): Linear(in_features=120, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    }
  ]
}